{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35129aef-ac12-4bf6-a697-95c46dc15898",
   "metadata": {},
   "source": [
    "## Exercise 1.4 | Accessing Web Data with Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9554a5-905d-4ac8-af52-ca291b300082",
   "metadata": {},
   "source": [
    "### Contents\n",
    "### 1. Importing Libraries\n",
    "### 2. Installing Chromedriver into the Notebook\n",
    "### 3. Scraping Data\n",
    "### 4. Saving scraped content as .txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948f4c4-f057-4068-af02-aea2fb2ec1bc",
   "metadata": {},
   "source": [
    "#### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf847bd-947c-4320-ac5b-9f80634b854c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing Libraries, as was instructed in exercise 1.4 (Task 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260094d8-7773-4763-9d45-f8ffe7d76c57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2315e8-9f87-4de5-b536-ee9458065126",
   "metadata": {},
   "source": [
    "#### 2. Installing Chromedriver into the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee208ead-78ac-44bc-8ae1-f4f0fe52b641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Installing the Google Chromedriver into the notebook. I've using the code suggested by the Exercise. (Task 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3000300-a570-4dea-8576-43b1df0a408d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service  =  Service(executable_path=r\"C:\\Program Files\\Google\\Chrome\\chromedriver-win64\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(executable_path=r\"C:\\Program Files\\Google\\Chrome\\chromedriver-win64\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6e4a9-2569-4c01-87a6-58158f1d3156",
   "metadata": {},
   "source": [
    "#### 3. Scraping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a4df2ee-0209-4851-9456-25f31015f852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scraping the data, as instructed in Exercise 1.4. We are pulling from https://en.wikipedia.org/wiki/Key_events_of_the_20th_century\n",
    "\n",
    "# Per the Project Brief (found below), we are to do the following:\n",
    "\n",
    "# Data Sourcing Criteria\n",
    "# Scrape a list of countries from a web page;\n",
    "# Scrape a twentieth-century history and politics page from Wikipedia.\n",
    "\n",
    "#Analysis Criteria\n",
    "# Use NLP to analyze the scraped content, compile a list of the countries involved in major twentieth-century events, and document how they’re connected.\n",
    "# Determine the degree, closeness, and betweenness centrality of the countries in your graph.\n",
    "\n",
    "#(https://coach-courses-us.s3.amazonaws.com/public/courses/da-spec-dv/A1/Data%20Visualizations%20with%20Python%20Achievement%201%20Project%20Brief%20Final%20%28CF%29.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12ead96e-db75-4105-808a-69b58ee984e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the page’s contents\n",
    "\n",
    "page_url = \"https://en.wikipedia.org/wiki/Key_events_of_the_20th_century\"\n",
    "driver.get(page_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca45ca0-c873-4779-934f-185631edf7b4",
   "metadata": {},
   "source": [
    "#### 4. Saving scraped content as .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc19c448-5734-4e84-a1c3-08469ee96649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now that we have scraped the desired webpage, we want to save it as a .txt file in our project folder. I will follow the instruction from Exercise 1.4 (Task 6)\n",
    "# First, we will import the libraries 'BeautifulSoup', and 'requests', these will help us complete this next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ad58857-6b7f-48e5-b759-870cb73fe12a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "011d43f2-332b-4e46-8bd1-1f1ccdb44d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get URL\n",
    "\n",
    "page_url =  requests.get(\"https://en.wikipedia.org/wiki/Key_events_of_the_20th_century\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9373c4de-fa1f-4fea-a65e-2841319f68cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Key events of the 20th century - Wikipedia</title>\n"
     ]
    }
   ],
   "source": [
    "# Create soup and get title\n",
    "\n",
    "soup = BeautifulSoup(page_url.text, 'html.parser')\n",
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69c74a4f-c422-4790-8843-351a57b1baea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The 2 previous lines of code have helped us establish what the page url is (page_url =  requests.get(\"https://en.wikipedia.org/wiki/Key_events_of_the_20th_century\"), \n",
    "# and then pull and create the title for our txt file:\n",
    "#(soup = BeautifulSoup(page_url.text, 'html.parser')\n",
    "#print(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68c4965e-6ad2-4cc9-bdf8-f33a04b3ea15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = soup.get_text()\n",
    "\n",
    "text = text.encode ('utf-8')\n",
    "\n",
    "with open('20th_century_scrape', 'wb') as f:\n",
    "       f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b61a0304-f3e7-407b-8e40-563503f22c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lastly, the content is saved, and titled, into our project folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:20th_century]",
   "language": "python",
   "name": "conda-env-20th_century-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
